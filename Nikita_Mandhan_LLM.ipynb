{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac3aa9bb-295a-4fb0-ae3d-715b70e9f78d",
   "metadata": {},
   "source": [
    "# Project : Named Entity Recognition (NER) & Parts of Speech (POS) tagging\n",
    "\n",
    "### Team Members : \n",
    "- **Nikita Baldev Mandhan**\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca4fcf-b115-4d03-8d40-b5a5322d4ffa",
   "metadata": {},
   "source": [
    "## Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff541d-2ac6-4438-b40f-77e8edbf25f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Library Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e45c9f4-8225-45c7-a61f-30beb03a3874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This script includes a range of imports for data manipulation, visualization, machine learning, and natural language processing. \n",
    "# NumPy and pandas are used for data handling and preprocessing, while matplotlib and seaborn are for data visualization. \n",
    "# LabelEncoder and train_test_split from sklearn help in preparing the data for modeling, and accuracy_score is used to evaluate model performance.\n",
    "# The keras preprocessing is used for preparing sequence data for deep learning models, and sklearn_crfsuite is for Conditional Random Fields\n",
    "# Warnings are controlled for clean output, and TensorFlow's utilities are used for categorical encoding. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import sklearn_crfsuite\n",
    "import warnings\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from seqeval.metrics import classification_report as seqeval_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e23365-ff4f-4dfd-9ff0-5fdb77577d0b",
   "metadata": {},
   "source": [
    "### Data Loading, Preprocessing, and Structuring for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516ffcd6-c281-4254-825e-9bd0701e67a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Read the dataset\n",
    "df = pd.read_csv('ner_dataset.csv', encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88efea70-a6bc-4086-baaa-c7fc0ddb95a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>NaN</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>NaN</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>NaN</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sentence #           Word  POS Tag\n",
       "0        Sentence: 1      Thousands  NNS   O\n",
       "1                NaN             of   IN   O\n",
       "2                NaN  demonstrators  NNS   O\n",
       "3                NaN           have  VBP   O\n",
       "4                NaN        marched  VBN   O\n",
       "...              ...            ...  ...  ..\n",
       "1048570          NaN           they  PRP   O\n",
       "1048571          NaN      responded  VBD   O\n",
       "1048572          NaN             to   TO   O\n",
       "1048573          NaN            the   DT   O\n",
       "1048574          NaN         attack   NN   O\n",
       "\n",
       "[1048575 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8b2f25-5c16-4ed3-a19d-f5901f953774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rename the sentence column for consistency.\n",
    "df.rename(columns={'Sentence #': 'Sentence#'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe55ac7-9471-4042-ac56-1016d64c6771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence#</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>conflict</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>joined</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protesters</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>who</td>\n",
       "      <td>WP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>carried</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>banners</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>such</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NaN</td>\n",
       "      <td>slogans</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence#           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1           NaN             of   IN      O\n",
       "2           NaN  demonstrators  NNS      O\n",
       "3           NaN           have  VBP      O\n",
       "4           NaN        marched  VBN      O\n",
       "5           NaN        through   IN      O\n",
       "6           NaN         London  NNP  B-geo\n",
       "7           NaN             to   TO      O\n",
       "8           NaN        protest   VB      O\n",
       "9           NaN            the   DT      O\n",
       "10          NaN            war   NN      O\n",
       "11          NaN             in   IN      O\n",
       "12          NaN           Iraq  NNP  B-geo\n",
       "13          NaN            and   CC      O\n",
       "14          NaN         demand   VB      O\n",
       "15          NaN            the   DT      O\n",
       "16          NaN     withdrawal   NN      O\n",
       "17          NaN             of   IN      O\n",
       "18          NaN        British   JJ  B-gpe\n",
       "19          NaN         troops  NNS      O\n",
       "20          NaN           from   IN      O\n",
       "21          NaN           that   DT      O\n",
       "22          NaN        country   NN      O\n",
       "23          NaN              .    .      O\n",
       "24  Sentence: 2       Families  NNS      O\n",
       "25          NaN             of   IN      O\n",
       "26          NaN       soldiers  NNS      O\n",
       "27          NaN         killed  VBN      O\n",
       "28          NaN             in   IN      O\n",
       "29          NaN            the   DT      O\n",
       "30          NaN       conflict   NN      O\n",
       "31          NaN         joined  VBD      O\n",
       "32          NaN            the   DT      O\n",
       "33          NaN     protesters  NNS      O\n",
       "34          NaN            who   WP      O\n",
       "35          NaN        carried  VBD      O\n",
       "36          NaN        banners  NNS      O\n",
       "37          NaN           with   IN      O\n",
       "38          NaN           such   JJ      O\n",
       "39          NaN        slogans  NNS      O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab91d1d-c4f3-4a08-82b1-f64eb2baaaff",
   "metadata": {},
   "source": [
    "##### Sentence Reconstruction and Data Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43b2a5f-8099-40a4-a22e-861902260059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to reconstruct sentences from tokenized words and their corresponding POS and tags.\n",
    "def reconstruct_sentences(data):\n",
    "    # Initialize containers for sentences, POS tags, and NER tags.\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    pos_tags = []\n",
    "    ner_tags = []\n",
    "    \n",
    "    # Iterate over each row in the dataframe to rebuild sentence structure.\n",
    "    for index, row in data.iterrows():\n",
    "        # New sentence detection, append the previous one if it exists.\n",
    "        if pd.notna(row['Sentence#']):\n",
    "            if sentence:\n",
    "                sentences.append((' '.join(sentence), ' '.join(pos_tags), ' '.join(ner_tags)))\n",
    "                sentence = []\n",
    "                pos_tags = []\n",
    "                ner_tags = []\n",
    "        word = row['Word']\n",
    "        pos = row['POS']\n",
    "        tag = row['Tag']\n",
    "        # Append word, POS tag, and NER tag to the current sentence if they are valid.\n",
    "        if pd.notna(word) and isinstance(word, str):\n",
    "            sentence.append(word)\n",
    "            pos_tags.append(pos)\n",
    "            ner_tags.append(tag)\n",
    "    \n",
    "    # Append the last sentence if it hasn't been added already.\n",
    "    if sentence:\n",
    "        sentences.append((' '.join(sentence), ' '.join(pos_tags), ' '.join(ner_tags)))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3e903b-36a7-453b-a612-1006e9e7e550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "POS tags: NNS IN NNS VBP VBN IN NNP TO VB DT NN IN NNP CC VB DT NN IN JJ NNS IN DT NN .\n",
      "NER tags: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n",
      "Sentence: Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \"\n",
      "POS tags: NNS IN NNS VBN IN DT NN VBD DT NNS WP VBD NNS IN JJ NNS IN `` NNP NN CD NN `` CC `` VB DT NNS . ``\n",
      "NER tags: O O O O O O O O O O O O O O O O O O B-per O O O O O O O O O O O\n",
      "\n",
      "Sentence: They marched from the Houses of Parliament to a rally in Hyde Park .\n",
      "POS tags: PRP VBD IN DT NNS IN NN TO DT NN IN NNP NNP .\n",
      "NER tags: O O O O O O O O O O O B-geo I-geo O\n",
      "\n",
      "Sentence: Police put the number of marchers at 10,000 while organizers claimed it was 1,00,000 .\n",
      "POS tags: NNS VBD DT NN IN NNS IN CD IN NNS VBD PRP VBD CD .\n",
      "NER tags: O O O O O O O O O O O O O O O\n",
      "\n",
      "Sentence: The protest comes on the eve of the annual conference of Britain 's ruling Labor Party in the southern English seaside resort of Brighton .\n",
      "POS tags: DT NN VBZ IN DT NN IN DT JJ NN IN NNP POS VBG NNP NNP IN DT JJ JJ NN NN IN NNP .\n",
      "NER tags: O O O O O O O O O O O B-geo O O B-org I-org O O O B-gpe O O O B-geo O\n",
      "\n",
      "Sentence: The party is divided over Britain 's participation in the Iraq conflict and the continued deployment of 8,500 British troops in that country .\n",
      "POS tags: DT NN VBZ VBN IN NNP POS NN IN DT NNP NN CC DT JJ NN IN CD JJ NNS IN DT NN .\n",
      "NER tags: O O O O O B-gpe O O O O B-geo O O O O O O O B-gpe O O O O O\n",
      "\n",
      "Sentence: The London march came ahead of anti-war protests today in other cities , including Rome , Paris , and Madrid .\n",
      "POS tags: DT NNP NN VBD RB IN JJ NNS NN IN JJ NNS , VBG NNP , NNP , CC NNP .\n",
      "NER tags: O B-geo O O O O O O O O O O O O B-geo O B-geo O O B-geo O\n",
      "\n",
      "Sentence: The International Atomic Energy Agency is to hold second day of talks in Vienna Wednesday on how to respond to Iran 's resumption of low-level uranium conversion .\n",
      "POS tags: DT NNP NNP NNP NNP VBZ TO VB JJ NN IN NNS IN NNP NNP IN WRB TO VB TO NNP POS NN IN JJ NN NN .\n",
      "NER tags: O B-org I-org I-org I-org O O O O O O O O B-geo B-tim O O O O O B-gpe O O O O O O O\n",
      "\n",
      "Sentence: Iran this week restarted parts of the conversion process at its Isfahan nuclear plant .\n",
      "POS tags: NNP DT NN VBD NNS IN DT NN NN IN PRP$ NNP JJ NN .\n",
      "NER tags: B-gpe O O O O O O O O O O B-geo O O O\n",
      "\n",
      "Sentence: Iranian officials say they expect to get access to sealed sensitive parts of the plant Wednesday , after an IAEA surveillance system begins functioning .\n",
      "POS tags: JJ NNS VBP PRP VBP TO VB NN TO JJ JJ NNS IN DT NN NNP , IN DT NNP NN NN VBZ VBG .\n",
      "NER tags: B-gpe O O O O O O O O O O O O O O B-tim O O O B-org O O O O O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = reconstruct_sentences(df)\n",
    "for sentence, pos, ner in sentences[:10]:\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"POS tags:\", pos)\n",
    "    print(\"NER tags:\", ner)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9ea0a3-b157-46f8-971d-d7bfc9eeb133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "('Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .', 'NNS IN NNS VBP VBN IN NNP TO VB DT NN IN NNP CC VB DT NN IN JJ NNS IN DT NN .', 'O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O')\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences[0]))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db5021b7-2121-452c-940a-5ea1362474d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract and prepare sentences and tags for further processing.\n",
    "sample_sentences = [sentence_tuple[0] for sentence_tuple in sentences]\n",
    "sample_POS_tags = [sentence_tuple[1] for sentence_tuple in sentences]\n",
    "sample_tags = [sentence_tuple[2] for sentence_tuple in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfbd24-5ec1-4b04-b2ce-a3768f3a747b",
   "metadata": {},
   "source": [
    "##### Data Preprocessing and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "804d4dfd-b298-4566-a5eb-4ae25ad3fd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize sentences and their corresponding tag sequences.\n",
    "tokenized_sentences = [sentence.split() for sentence in sample_sentences]\n",
    "tokenized_POS_tags = [POS_tag_sequence.split() for POS_tag_sequence in sample_POS_tags]\n",
    "tokenized_tags = [tag_sequence.split() for tag_sequence in sample_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abefd318-38f0-4820-97c0-5e54e18edeae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 47959\n",
      "Total number of unique words: 35175\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of sentences and unique words.\n",
    "total_sentences = len(tokenized_sentences)\n",
    "print(f\"Total number of sentences: {total_sentences}\")\n",
    "\n",
    "unique_words = set(word for sentence in tokenized_sentences for word in sentence)\n",
    "print(f\"Total number of unique words: {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b15168f-c805-4ae1-bef6-9e8a7cdadd66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter out sentences where the number of tokens does not match the number of tags.\n",
    "filtered_sentences = []\n",
    "filtered_POS_tags = []\n",
    "filtered_tags = []\n",
    "\n",
    "for tokens, POS_tags, tags in zip(tokenized_sentences, tokenized_POS_tags, tokenized_tags):\n",
    "    if len(tokens) == len(POS_tags) == len(tags):\n",
    "        filtered_sentences.append(tokens)\n",
    "        filtered_POS_tags.append(POS_tags)\n",
    "        filtered_tags.append(tags)\n",
    "        \n",
    "# Update tokenized sentences and tags with the filtered data.\n",
    "tokenized_sentences = filtered_sentences\n",
    "tokenized_POS_tags = filtered_POS_tags\n",
    "tokenized_tags = filtered_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a24d20e-4f74-41cf-95e4-f721ba22a45b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.'], ['Families', 'of', 'soldiers', 'killed', 'in', 'the', 'conflict', 'joined', 'the', 'protesters', 'who', 'carried', 'banners', 'with', 'such', 'slogans', 'as', '\"', 'Bush', 'Number', 'One', 'Terrorist', '\"', 'and', '\"', 'Stop', 'the', 'Bombings', '.', '\"'], ['They', 'marched', 'from', 'the', 'Houses', 'of', 'Parliament', 'to', 'a', 'rally', 'in', 'Hyde', 'Park', '.'], ['Police', 'put', 'the', 'number', 'of', 'marchers', 'at', '10,000', 'while', 'organizers', 'claimed', 'it', 'was', '1,00,000', '.'], ['The', 'protest', 'comes', 'on', 'the', 'eve', 'of', 'the', 'annual', 'conference', 'of', 'Britain', \"'s\", 'ruling', 'Labor', 'Party', 'in', 'the', 'southern', 'English', 'seaside', 'resort', 'of', 'Brighton', '.'], ['The', 'party', 'is', 'divided', 'over', 'Britain', \"'s\", 'participation', 'in', 'the', 'Iraq', 'conflict', 'and', 'the', 'continued', 'deployment', 'of', '8,500', 'British', 'troops', 'in', 'that', 'country', '.'], ['The', 'London', 'march', 'came', 'ahead', 'of', 'anti-war', 'protests', 'today', 'in', 'other', 'cities', ',', 'including', 'Rome', ',', 'Paris', ',', 'and', 'Madrid', '.'], ['The', 'International', 'Atomic', 'Energy', 'Agency', 'is', 'to', 'hold', 'second', 'day', 'of', 'talks', 'in', 'Vienna', 'Wednesday', 'on', 'how', 'to', 'respond', 'to', 'Iran', \"'s\", 'resumption', 'of', 'low-level', 'uranium', 'conversion', '.'], ['Iran', 'this', 'week', 'restarted', 'parts', 'of', 'the', 'conversion', 'process', 'at', 'its', 'Isfahan', 'nuclear', 'plant', '.'], ['Iranian', 'officials', 'say', 'they', 'expect', 'to', 'get', 'access', 'to', 'sealed', 'sensitive', 'parts', 'of', 'the', 'plant', 'Wednesday', ',', 'after', 'an', 'IAEA', 'surveillance', 'system', 'begins', 'functioning', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c7c3dd9-ac35-4e04-ad59-c6ba4b7996ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'CC', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.'], ['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'VBD', 'DT', 'NNS', 'WP', 'VBD', 'NNS', 'IN', 'JJ', 'NNS', 'IN', '``', 'NNP', 'NN', 'CD', 'NN', '``', 'CC', '``', 'VB', 'DT', 'NNS', '.', '``'], ['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', 'TO', 'DT', 'NN', 'IN', 'NNP', 'NNP', '.'], ['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', 'CD', 'IN', 'NNS', 'VBD', 'PRP', 'VBD', 'CD', '.'], ['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'POS', 'VBG', 'NNP', 'NNP', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'NNP', '.'], ['DT', 'NN', 'VBZ', 'VBN', 'IN', 'NNP', 'POS', 'NN', 'IN', 'DT', 'NNP', 'NN', 'CC', 'DT', 'JJ', 'NN', 'IN', 'CD', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.'], ['DT', 'NNP', 'NN', 'VBD', 'RB', 'IN', 'JJ', 'NNS', 'NN', 'IN', 'JJ', 'NNS', ',', 'VBG', 'NNP', ',', 'NNP', ',', 'CC', 'NNP', '.'], ['DT', 'NNP', 'NNP', 'NNP', 'NNP', 'VBZ', 'TO', 'VB', 'JJ', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'NNP', 'IN', 'WRB', 'TO', 'VB', 'TO', 'NNP', 'POS', 'NN', 'IN', 'JJ', 'NN', 'NN', '.'], ['NNP', 'DT', 'NN', 'VBD', 'NNS', 'IN', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NNP', 'JJ', 'NN', '.'], ['JJ', 'NNS', 'VBP', 'PRP', 'VBP', 'TO', 'VB', 'NN', 'TO', 'JJ', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'NNP', ',', 'IN', 'DT', 'NNP', 'NN', 'NN', 'VBZ', 'VBG', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_POS_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199f48a0-935a-4a3e-90a7-99a478fae76d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'B-org', 'I-org', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'B-geo', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'B-geo', 'O', 'O', 'B-geo', 'O'], ['O', 'B-org', 'I-org', 'I-org', 'I-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O'], ['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24146cd8-97b5-4adf-b0e3-f66521d16639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 47955\n",
      "Total number of unique words: 35168\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of sentences and unique words after removing mismatches\n",
    "total_sentences = len(tokenized_sentences)\n",
    "print(f\"Total number of sentences: {total_sentences}\")\n",
    "\n",
    "unique_words = set(word for sentence in tokenized_sentences for word in sentence)\n",
    "print(f\"Total number of unique words: {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cad2e-05b4-4f68-bfb2-5184e45e420e",
   "metadata": {},
   "source": [
    "**Note:** In the initial dataset for Named Entity Recognition (NER), we had 47,959 sentences with 35,176 unique words. However, during data validation, we identified sentences where the number of tokens didn't match the corresponding tags. Post-cleaning, the dataset contained 47,955 sentences and 35,169 unique words, indicating that 4 sentences were removed due to mismatches. Ensuring data alignment is crucial for effective model training and to avoid potential errors during model evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0faf2e-9c82-4d98-bd51-f282676f8fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vocabulary and tags setup for neural network models\n",
    "words = [\"<PAD>\"] + list(set(word for sentence in tokenized_sentences for word in sentence))\n",
    "tags_set = list(set(tag for tag_sequence in tokenized_tags for tag in tag_sequence))\n",
    "# Setup for POS tags\n",
    "pos_tags_set = list(set(pos_tag for pos_tag_sequence in tokenized_POS_tags for pos_tag in pos_tag_sequence))\n",
    "\n",
    "\n",
    "# Mappings from word/tag to a unique index\n",
    "word2idx = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "# Mappings for NER tags\n",
    "tag2idx = {tag: i for i, tag in enumerate(tags_set)}\n",
    "idx2tag = {i: tag for tag, i in tag2idx.items()}\n",
    "\n",
    "# Mappings for POS tags\n",
    "pos_tag2idx = {pos_tag: i for i, pos_tag in enumerate(pos_tags_set)}\n",
    "pos_tag2idx['<PAD>'] = len(pos_tag2idx)  # Add a padding token\n",
    "idx2pos_tag = {i: pos_tag for pos_tag, i in pos_tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3c41cbe-5fff-4706-bc58-e68b28f33e49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<PAD>', 0), ('poked', 1), ('Fonda', 2), ('proof', 3), ('WPMF-TV', 4), ('Ilya', 5), ('watching', 6), ('inserting', 7), ('translated', 8), ('Popularly-elected', 9)]\n",
      "[('B-geo', 0), ('I-org', 1), ('B-art', 2), ('I-nat', 3), ('B-gpe', 4), ('B-org', 5), ('B-nat', 6), ('B-eve', 7), ('I-eve', 8), ('I-tim', 9), ('I-gpe', 10), ('B-per', 11), ('I-per', 12), ('O', 13), ('I-geo', 14), ('B-tim', 15), ('I-art', 16)]\n",
      "[(0, 'B-geo'), (1, 'I-org'), (2, 'B-art'), (3, 'I-nat'), (4, 'B-gpe'), (5, 'B-org'), (6, 'B-nat'), (7, 'B-eve'), (8, 'I-eve'), (9, 'I-tim'), (10, 'I-gpe'), (11, 'B-per'), (12, 'I-per'), (13, 'O'), (14, 'I-geo'), (15, 'B-tim'), (16, 'I-art')]\n",
      "[('JJ', 0), ('NNS', 1), ('VBZ', 2), ('CD', 3), ('POS', 4), ('PDT', 5), ('EX', 6), (',', 7), ('RRB', 8), (':', 9), ('MD', 10), (';', 11), ('WDT', 12), ('WP$', 13), ('VB', 14), ('VBG', 15), ('WRB', 16), ('NNP', 17), ('PRP', 18), ('PRP$', 19), ('JJS', 20), ('VBN', 21), ('NN', 22), ('``', 23), ('.', 24), ('RP', 25), ('VBP', 26), ('$', 27), ('DT', 28), ('WP', 29), ('UH', 30), ('CC', 31), ('RBR', 32), ('VBD', 33), ('TO', 34), ('FW', 35), ('IN', 36), ('JJR', 37), ('NNPS', 38), ('LRB', 39), ('RBS', 40), ('RB', 41), ('<PAD>', 42)]\n",
      "[(0, 'JJ'), (1, 'NNS'), (2, 'VBZ'), (3, 'CD'), (4, 'POS'), (5, 'PDT'), (6, 'EX'), (7, ','), (8, 'RRB'), (9, ':'), (10, 'MD'), (11, ';'), (12, 'WDT'), (13, 'WP$'), (14, 'VB'), (15, 'VBG'), (16, 'WRB'), (17, 'NNP'), (18, 'PRP'), (19, 'PRP$'), (20, 'JJS'), (21, 'VBN'), (22, 'NN'), (23, '``'), (24, '.'), (25, 'RP'), (26, 'VBP'), (27, '$'), (28, 'DT'), (29, 'WP'), (30, 'UH'), (31, 'CC'), (32, 'RBR'), (33, 'VBD'), (34, 'TO'), (35, 'FW'), (36, 'IN'), (37, 'JJR'), (38, 'NNPS'), (39, 'LRB'), (40, 'RBS'), (41, 'RB'), (42, '<PAD>')]\n"
     ]
    }
   ],
   "source": [
    "# Print first 10 items from word2idx\n",
    "print(list(word2idx.items())[:10])\n",
    "\n",
    "# Print items from tag2idx\n",
    "print(list(tag2idx.items()))\n",
    "\n",
    "# Print items from idx2tag\n",
    "print(list(idx2tag.items()))\n",
    "\n",
    "# Print items from pos_tag2idx\n",
    "print(list(pos_tag2idx.items()))\n",
    "\n",
    "# Print items from idx2pos_tag\n",
    "print(list(idx2pos_tag.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249ccbb-2467-4f60-8927-fa52e8583533",
   "metadata": {},
   "source": [
    "### Utility Functions for Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c686bde1-fa5a-4a94-b87f-8168aa1f2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert tags to indices\n",
    "def tags_to_indices(tags, tag2idx):\n",
    "    return [[tag2idx.get(tag) for tag in tag_list] for tag_list in tags]\n",
    "\n",
    "# Convert NER and POS tags to their respective indices\n",
    "tokenized_tags_indices = tags_to_indices(tokenized_tags, tag2idx)\n",
    "tokenized_POS_tags_indices = tags_to_indices(tokenized_POS_tags, pos_tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e4e5996-68f3-4abc-b297-ded7dea63100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align tags with tokenized inputs\n",
    "def align_tags(tags, tokenized_inputs):\n",
    "    aligned_tags = []\n",
    "    for i, label_ids in enumerate(tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids_aligned = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids_aligned.append(-100)  # Special token\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids_aligned.append(label_ids[word_idx])  # Word start\n",
    "            else:\n",
    "                label_ids_aligned.append(-100)  # Use -100 for subwords\n",
    "            previous_word_idx = word_idx\n",
    "        aligned_tags.append(label_ids_aligned)\n",
    "    return aligned_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe4b96-104f-4294-b405-8a535309a25d",
   "metadata": {},
   "source": [
    "### Training and Evaluation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ffc1995-0492-4e0f-b56d-e38e4cae6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, optimizer, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    progress_bar_train = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar_train:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Flatten predictions and labels for computing accuracy\n",
    "        predictions = predictions.view(-1)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Ignore index -100 (used for special tokens)\n",
    "        mask = labels != -100\n",
    "        predictions = predictions[mask]\n",
    "        labels = labels[mask]\n",
    "\n",
    "        total_accuracy += (predictions == labels).sum().item() / labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_accuracy = total_accuracy / len(dataloader)\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c8b17ba-f9b6-467d-b5eb-8a361b10d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, loss_fn, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    progress_bar_eval = tqdm(dataloader, desc=\"Evaluating\")\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for batch in progress_bar_eval:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(outputs.logits, dim=2)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = predictions.view(-1)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            mask = labels != -100\n",
    "            predictions = predictions[mask]\n",
    "            labels = labels[mask]\n",
    "\n",
    "            total_accuracy += (predictions == labels).sum().item() / labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_accuracy = total_accuracy / len(dataloader)\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060dfd93-d15c-4477-a43f-f7c7e1ae98cc",
   "metadata": {},
   "source": [
    "### Reporting Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb6b0adf-aeac-4b4a-ad3b-b10c707d0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_generate_report(model, dataloader, idx2tag, is_ner=True):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            pred_tags = torch.argmax(logits, dim=2)\n",
    "            pred_tags = pred_tags.cpu().numpy()\n",
    "\n",
    "            for i, label in enumerate(labels.cpu().numpy()):\n",
    "                if is_ner:\n",
    "                    # NER: Skip padding tokens\n",
    "                    token_labels = [idx2tag[t] for t, l in zip(pred_tags[i], label) if l != -100]\n",
    "                    true_labels.append([idx2tag[l] for l in label if l != -100])\n",
    "                else:\n",
    "                    # POS: Skip padding tokens\n",
    "                    token_labels = [idx2tag[t] for t, l in zip(pred_tags[i], label) if l != -100]\n",
    "                    true_labels.append([idx2tag[l] for l in label if l != -100])\n",
    "\n",
    "                predictions.append(token_labels)\n",
    "\n",
    "    if is_ner:\n",
    "        report = seqeval_report(true_labels, predictions)\n",
    "    else:\n",
    "        flat_true = [item for sublist in true_labels for item in sublist]\n",
    "        flat_pred = [item for sublist in predictions for item in sublist]\n",
    "        report = classification_report(flat_true, flat_pred)\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e1247-cc97-463b-ad49-4208f71b5f18",
   "metadata": {},
   "source": [
    "## Transformer Models\n",
    "### ---- 1. XLNet LLM ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f309c9d-4180-4a24-bab1-57081f46da38",
   "metadata": {},
   "source": [
    "#### XLNet Tokenizer and Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "151f781d-693b-4b88-beb3-467cfed0d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizerFast, XLNetForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91f0bdc1-e690-4efe-b38b-8947046d82c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForTokenClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLNetForTokenClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "xlnet_tokenizer = XLNetTokenizerFast.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Initialize XLNet model for Named Entity Recognition (NER)\n",
    "xlnet_model_ner = XLNetForTokenClassification.from_pretrained('xlnet-base-cased', num_labels=len(tags_set))\n",
    "\n",
    "# Initialize XLNet model for Part-of-Speech (POS) tagging\n",
    "xlnet_model_pos = XLNetForTokenClassification.from_pretrained('xlnet-base-cased', num_labels=len(pos_tags_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447c36b5-d972-40cf-bb2d-360f75201a52",
   "metadata": {},
   "source": [
    "#### Tokenization and Alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82809aac-023f-4343-b2fc-1a0922e9d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize for NER\n",
    "tokenized_inputs_ner = xlnet_tokenizer(tokenized_sentences, padding=True, truncation=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize for POS tagging\n",
    "tokenized_inputs_pos = xlnet_tokenizer(tokenized_sentences, padding=True, truncation=True, is_split_into_words=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11bf300f-1ccb-40a9-9180-b3f240c10f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-align tags after converting them to indices\n",
    "aligned_tags_ner = align_tags(tokenized_tags_indices, tokenized_inputs_ner)\n",
    "aligned_tags_pos = align_tags(tokenized_POS_tags_indices, tokenized_inputs_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf81be-1c6d-4b02-b5fa-bbc54b34e287",
   "metadata": {},
   "source": [
    "#### Conversion to Tensors and Dataset Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81588e1b-f109-446c-bcf6-996ef2d1998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert aligned tags to tensors for NER\n",
    "labels_ner = torch.tensor(aligned_tags_ner)\n",
    "\n",
    "# Convert aligned tags to tensors for POS tagging\n",
    "labels_pos = torch.tensor(aligned_tags_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "690c7b25-bb1c-413d-85e6-92e99057a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Create the TensorDataset for NER\n",
    "dataset_ner = TensorDataset(tokenized_inputs_ner['input_ids'], tokenized_inputs_ner['attention_mask'], labels_ner)\n",
    "\n",
    "# Create the TensorDataset for POS tagging\n",
    "dataset_pos = TensorDataset(tokenized_inputs_pos['input_ids'], tokenized_inputs_pos['attention_mask'], labels_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc1003-d841-456a-8055-15d976ef928e",
   "metadata": {},
   "source": [
    "#### Optimizer and Loss Function Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5153d74-5a2f-4781-95e7-185d09124b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Define the optimizer using PyTorch's AdamW\n",
    "optimizer_ner = AdamW(xlnet_model_ner.parameters(), lr=5e-5)\n",
    "optimizer_pos = AdamW(xlnet_model_pos.parameters(), lr=5e-5)\n",
    "\n",
    "# Loss Functions for NER and POS Tagging\n",
    "loss_fn_ner = torch.nn.CrossEntropyLoss()\n",
    "loss_fn_pos = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c02fa-bc8b-44c0-a495-ea2f74679351",
   "metadata": {},
   "source": [
    "#### DataLoaders and Training/Validation Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "871a2153-38d2-438c-b5c1-12c9fa713143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, random_split\n",
    "\n",
    "# Split data into train and validation sets for NER\n",
    "val_size_ner = int(len(dataset_ner) * 0.2)\n",
    "train_size_ner = len(dataset_ner) - val_size_ner\n",
    "train_dataset_ner, val_dataset_ner = random_split(dataset_ner, [train_size_ner, val_size_ner])\n",
    "\n",
    "# Split data into train and validation sets for POS\n",
    "val_size_pos = int(len(dataset_pos) * 0.2)\n",
    "train_size_pos = len(dataset_pos) - val_size_pos\n",
    "train_dataset_pos, val_dataset_pos = random_split(dataset_pos, [train_size_pos, val_size_pos])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataloader_ner = DataLoader(train_dataset_ner, sampler=RandomSampler(train_dataset_ner), batch_size=batch_size)\n",
    "val_dataloader_ner = DataLoader(val_dataset_ner, batch_size=batch_size)\n",
    "\n",
    "train_dataloader_pos = DataLoader(train_dataset_pos, sampler=RandomSampler(train_dataset_pos), batch_size=batch_size)\n",
    "val_dataloader_pos = DataLoader(val_dataset_pos, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e8111-f8b5-4998-83ef-49543ed348d2",
   "metadata": {},
   "source": [
    "#### Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95bf727b-be14-451a-836d-5520c3648082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Evaluating NER Model\n",
      "Epoch 1/3 - NER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [18:29<00:00,  1.08it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n",
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER - Train Loss: 0.1469, Train Accuracy: 0.9587, Val Loss: 0.0995, Val Accuracy: 0.9685\n",
      "Epoch 2/3 - NER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [18:28<00:00,  1.08it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n",
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER - Train Loss: 0.0895, Train Accuracy: 0.9715, Val Loss: 0.0936, Val Accuracy: 0.9712\n",
      "Epoch 3/3 - NER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [18:30<00:00,  1.08it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n",
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER - Train Loss: 0.0714, Train Accuracy: 0.9765, Val Loss: 0.0922, Val Accuracy: 0.9717\n",
      "\n",
      "Training and Evaluating POS Tagging Model\n",
      "Epoch 1/3 - POS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [18:31<00:00,  1.08it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n",
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS - Train Loss: 0.1796, Train Accuracy: 0.9514, Val Loss: 0.0482, Val Accuracy: 0.9853\n",
      "Epoch 2/3 - POS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [18:31<00:00,  1.08it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n",
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS - Train Loss: 0.0465, Train Accuracy: 0.9853, Val Loss: 0.0445, Val Accuracy: 0.9863\n",
      "Epoch 3/3 - POS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [18:31<00:00,  1.08it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [02:03<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS - Train Loss: 0.0360, Train Accuracy: 0.9882, Val Loss: 0.0454, Val Accuracy: 0.9864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose the device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the specified device\n",
    "xlnet_model_ner.to(device)\n",
    "xlnet_model_pos.to(device)\n",
    "\n",
    "num_epochs=3\n",
    "# Main training and validation loop for NER\n",
    "print(\"Training and Evaluating NER Model\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - NER\")\n",
    "\n",
    "    # Training\n",
    "    train_loss_ner, train_acc_ner = train_model(xlnet_model_ner, optimizer_ner, loss_fn_ner, train_dataloader_ner, device)\n",
    "    # Validation\n",
    "    val_loss_ner, val_acc_ner = evaluate_model(xlnet_model_ner, loss_fn_ner, val_dataloader_ner, device)\n",
    "\n",
    "    tqdm.write(f\"NER - Train Loss: {train_loss_ner:.4f}, Train Accuracy: {train_acc_ner:.4f}, Val Loss: {val_loss_ner:.4f}, Val Accuracy: {val_acc_ner:.4f}\")\n",
    "\n",
    "# Main training and validation loop for POS tagging\n",
    "print(\"\\nTraining and Evaluating POS Tagging Model\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - POS\")\n",
    "\n",
    "    # Training\n",
    "    train_loss_pos, train_acc_pos = train_model(xlnet_model_pos, optimizer_pos, loss_fn_pos, train_dataloader_pos, device)\n",
    "    # Validation\n",
    "    val_loss_pos, val_acc_pos = evaluate_model(xlnet_model_pos, loss_fn_pos, val_dataloader_pos, device)\n",
    "\n",
    "    tqdm.write(f\"POS - Train Loss: {train_loss_pos:.4f}, Train Accuracy: {train_acc_pos:.4f}, Val Loss: {val_loss_pos:.4f}, Val Accuracy: {val_acc_pos:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b25ef-92e8-43cc-a19d-416d9c95fc7a",
   "metadata": {},
   "source": [
    "#### Inference and Reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "763163ed-efec-4900-bf5b-eefc9eabf05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack - NER: B-per, POS: NNP\n",
      "Obama - NER: I-per, POS: NNP\n",
      "visited - NER: O, POS: VBD\n",
      "London - NER: B-geo, POS: NNP\n",
      "in - NER: O, POS: IN\n",
      "2015 - NER: B-tim, POS: CD\n",
      "and - NER: O, POS: CC\n",
      "spoke - NER: O, POS: VBD\n",
      "at - NER: O, POS: IN\n",
      "the - NER: O, POS: DT\n",
      "University - NER: B-org, POS: NNP\n",
      "of - NER: I-org, POS: IN\n",
      "Cambridge - NER: I-org, POS: NNP\n",
      ". - NER: O, POS: PUNCT\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Define the sentence\n",
    "sentence = \"Barack Obama visited London in 2015 and spoke at the University of Cambridge.\"\n",
    "\n",
    "# Tokenize the sentence using XLNet tokenizer\n",
    "inputs = xlnet_tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "# Perform inference for NER and POS tagging\n",
    "with torch.no_grad():\n",
    "    outputs_ner = xlnet_model_ner(input_ids, attention_mask=attention_mask)\n",
    "    outputs_pos = xlnet_model_pos(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get the predicted tags\n",
    "predictions_ner = torch.argmax(outputs_ner.logits, dim=2)\n",
    "predictions_pos = torch.argmax(outputs_pos.logits, dim=2)\n",
    "\n",
    "# Decode predictions into tags\n",
    "ner_tags = [idx2tag[idx] for idx in predictions_ner[0].cpu().numpy()]\n",
    "pos_tags = [idx2pos_tag[idx] for idx in predictions_pos[0].cpu().numpy()]\n",
    "\n",
    "# Decode tokens into words\n",
    "tokenized_words = xlnet_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Align words with their tags and handle special cases\n",
    "processed_sentence = []\n",
    "for word, ner_tag, pos_tag in zip(tokenized_words, ner_tags, pos_tags):\n",
    "    # Skip special tokens\n",
    "    if word in [\"<sep>\", \"<cls>\", \"<pad>\"]:\n",
    "        continue\n",
    "\n",
    "    # Remove the \"▁\" symbol used by XLNet\n",
    "    if word.startswith(\"▁\"):\n",
    "        word = word[1:]\n",
    "\n",
    "    # Handle punctuation\n",
    "    if word in string.punctuation:\n",
    "        processed_sentence.append((word, \"O\", \"PUNCT\"))  # PUNCT is a generic POS tag for punctuation\n",
    "    else:\n",
    "        processed_sentence.append((word, ner_tag, pos_tag))\n",
    "\n",
    "# Print the words with their processed tags\n",
    "for word, ner_tag, pos_tag in processed_sentence:\n",
    "    print(f\"{word} - NER: {ner_tag}, POS: {pos_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08f5cf19-26e7-4d6e-b9c2-4b59df0f36bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         art       0.32      0.20      0.24        90\n",
      "         eve       0.37      0.28      0.32        61\n",
      "         geo       0.83      0.92      0.87      7571\n",
      "         gpe       0.95      0.94      0.95      3100\n",
      "         nat       0.41      0.33      0.37        36\n",
      "         org       0.75      0.66      0.71      4044\n",
      "         per       0.78      0.79      0.79      3450\n",
      "         tim       0.84      0.88      0.86      4011\n",
      "\n",
      "   micro avg       0.83      0.84      0.83     22363\n",
      "   macro avg       0.66      0.63      0.64     22363\n",
      "weighted avg       0.82      0.84      0.83     22363\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate report for NER\n",
    "ner_report_XLNet = predict_and_generate_report(xlnet_model_ner, val_dataloader_ner, idx2tag)\n",
    "print(\"NER Report:\\n\", ner_report_XLNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d177af23-60b6-45c5-9d86-d8fa8bd0a39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           $       1.00      1.00      1.00       228\n",
      "           ,       1.00      1.00      1.00      6458\n",
      "           .       1.00      1.00      1.00      9561\n",
      "           :       0.79      0.96      0.87       177\n",
      "           ;       0.93      0.97      0.95        38\n",
      "          CC       1.00      1.00      1.00      4753\n",
      "          CD       1.00      1.00      1.00      4905\n",
      "          DT       1.00      1.00      1.00     19792\n",
      "          EX       0.98      0.98      0.98       162\n",
      "          IN       1.00      0.99      1.00     24071\n",
      "          JJ       0.95      0.98      0.96     15465\n",
      "         JJR       0.95      0.96      0.96       594\n",
      "         JJS       0.97      0.98      0.97       590\n",
      "         LRB       1.00      1.00      1.00       123\n",
      "          MD       1.00      1.00      1.00      1475\n",
      "          NN       0.99      0.98      0.98     29042\n",
      "         NNP       0.99      0.98      0.98     25969\n",
      "        NNPS       0.86      0.87      0.87       509\n",
      "         NNS       0.99      0.99      0.99     15266\n",
      "         PDT       0.83      0.71      0.77        28\n",
      "         POS       1.00      1.00      1.00      2155\n",
      "         PRP       1.00      1.00      1.00      2679\n",
      "        PRP$       1.00      1.00      1.00      1750\n",
      "          RB       0.98      0.96      0.97      4006\n",
      "         RBR       0.86      0.84      0.85       209\n",
      "         RBS       0.85      0.85      0.85        62\n",
      "          RP       0.91      0.93      0.92       502\n",
      "         RRB       1.00      1.00      1.00       123\n",
      "          TO       0.99      1.00      1.00      4599\n",
      "          UH       0.40      1.00      0.57         2\n",
      "          VB       0.98      0.99      0.98      4941\n",
      "         VBD       0.99      0.97      0.98      7936\n",
      "         VBG       0.98      0.97      0.97      3793\n",
      "         VBN       0.95      0.98      0.96      6398\n",
      "         VBP       0.98      0.99      0.98      3333\n",
      "         VBZ       0.99      0.99      0.99      5002\n",
      "         WDT       0.98      0.98      0.98       719\n",
      "          WP       1.00      1.00      1.00       511\n",
      "         WP$       1.00      1.00      1.00        22\n",
      "         WRB       1.00      1.00      1.00       428\n",
      "          ``       1.00      1.00      1.00       747\n",
      "\n",
      "    accuracy                           0.99    209123\n",
      "   macro avg       0.95      0.97      0.96    209123\n",
      "weighted avg       0.99      0.99      0.99    209123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate report for POS\n",
    "pos_report_XLNet = predict_and_generate_report(xlnet_model_pos, val_dataloader_pos, idx2pos_tag, is_ner=False)\n",
    "print(\"POS Tagging Report:\\n\", pos_report_XLNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdea58d-dc56-4faa-95ab-24746958e84f",
   "metadata": {},
   "source": [
    "### ---- 2.  BERT ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ca411-646c-4ca4-8b8f-7bf7954c7beb",
   "metadata": {},
   "source": [
    "#### BERT Tokenizer and Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7b710fd-4c68-4ed0-8da3-ce9fd4974218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from seqeval.metrics import classification_report as seqeval_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1b7cfa8-2521-4ec4-9552-642b0cdc6896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer and model for NER and POS\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "bert_model_ner = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(tag2idx))\n",
    "bert_model_pos = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(pos_tag2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0147fc0-3041-46bd-a8b5-196458b8f3d6",
   "metadata": {},
   "source": [
    "#### Tokenization and Alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fa89942-7b7c-4e64-a170-4dec79ba6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize for NER and POS tagging\n",
    "tokenized_inputs_ner = bert_tokenizer(tokenized_sentences, padding=True, truncation=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "tokenized_inputs_pos = bert_tokenizer(tokenized_sentences, padding=True, truncation=True, is_split_into_words=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11c1b2f3-1b6d-4d4b-b8ea-7a1ce95956a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align tags for NER and POS\n",
    "aligned_tags_ner = align_tags(tokenized_tags_indices, tokenized_inputs_ner)\n",
    "aligned_tags_pos = align_tags(tokenized_POS_tags_indices, tokenized_inputs_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d5178-59b8-48fe-9911-87219d3ae8e2",
   "metadata": {},
   "source": [
    "#### Conversion to Tensors and Dataset Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d23b782-7e70-4a24-a70b-ce9a76a5b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TensorDataset for NER and POS tagging\n",
    "dataset_ner = TensorDataset(tokenized_inputs_ner['input_ids'], tokenized_inputs_ner['attention_mask'], torch.tensor(aligned_tags_ner))\n",
    "dataset_pos = TensorDataset(tokenized_inputs_pos['input_ids'], tokenized_inputs_pos['attention_mask'], torch.tensor(aligned_tags_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bbba7c-cb04-442c-beba-849a04b24f8c",
   "metadata": {},
   "source": [
    "#### Optimizer and Loss Function Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec30eb10-1e9b-4a00-ad88-97fc0336a063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venugopalbalamurug.l/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and loss function\n",
    "optimizer_ner = AdamW(bert_model_ner.parameters(), lr=5e-5)\n",
    "optimizer_pos = AdamW(bert_model_pos.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49f26f-0c4c-4a33-9ff5-1ef78c2d3b89",
   "metadata": {},
   "source": [
    "#### DataLoaders and Training/Validation Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efb75678-204f-4108-9ce2-2ac62c6bd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_dataset_ner, val_dataset_ner = random_split(dataset_ner, [int(len(dataset_ner) * 0.8), len(dataset_ner) - int(len(dataset_ner) * 0.8)])\n",
    "train_dataset_pos, val_dataset_pos = random_split(dataset_pos, [int(len(dataset_pos) * 0.8), len(dataset_pos) - int(len(dataset_pos) * 0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9092095-143d-418d-8454-51a84de3a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader_ner = DataLoader(train_dataset_ner, sampler=RandomSampler(train_dataset_ner), batch_size=32)\n",
    "val_dataloader_ner = DataLoader(val_dataset_ner, batch_size=32)\n",
    "train_dataloader_pos = DataLoader(train_dataset_pos, sampler=RandomSampler(train_dataset_pos), batch_size=32)\n",
    "val_dataloader_pos = DataLoader(val_dataset_pos, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "831fdf95-5fde-452a-9803-3c4056329bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 30 20:12:27 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-12GB            Off| 00000000:03:00.0 Off |                    0 |\n",
      "| N/A   52C    P0               34W / 250W|  10656MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE-12GB            Off| 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   33C    P0               25W / 250W|      2MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE-12GB            Off| 00000000:82:00.0 Off |                    0 |\n",
      "| N/A   33C    P0               25W / 250W|      2MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE-12GB            Off| 00000000:83:00.0 Off |                    0 |\n",
      "| N/A   32C    P0               24W / 250W|      2MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1280      C   ...entos7/anaconda3/2021.05/bin/python    10654MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae13d9a-692d-4bf4-8b77-1efab311e0cd",
   "metadata": {},
   "source": [
    "#### Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33e1d04f-457c-4b6c-aa59-aa1ad57994d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Evaluating NER Model\n",
      "Epoch 1/3 - NER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [10:18<00:00,  1.94it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [00:53<00:00,  5.60it/s]\n",
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER - Train Loss: 0.1328, Train Accuracy: 0.9625, Val Loss: 0.0897, Val Accuracy: 0.9714\n",
      "Epoch 2/3 - NER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [10:18<00:00,  1.94it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [00:53<00:00,  5.60it/s]\n",
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER - Train Loss: 0.0772, Train Accuracy: 0.9747, Val Loss: 0.0868, Val Accuracy: 0.9723\n",
      "Epoch 3/3 - NER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [10:18<00:00,  1.94it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [00:53<00:00,  5.60it/s]\n",
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER - Train Loss: 0.0570, Train Accuracy: 0.9804, Val Loss: 0.0897, Val Accuracy: 0.9729\n",
      "\n",
      "Training and Evaluating POS Tagging Model\n",
      "Epoch 1/3 - POS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [10:18<00:00,  1.94it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [00:53<00:00,  5.61it/s]\n",
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS - Train Loss: 0.1159, Train Accuracy: 0.9701, Val Loss: 0.0479, Val Accuracy: 0.9853\n",
      "Epoch 2/3 - POS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [10:17<00:00,  1.94it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [00:53<00:00,  5.61it/s]\n",
      "Training:   0%|          | 0/1199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS - Train Loss: 0.0380, Train Accuracy: 0.9877, Val Loss: 0.0451, Val Accuracy: 0.9864\n",
      "Epoch 3/3 - POS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1199/1199 [10:18<00:00,  1.94it/s]\n",
      "Evaluating: 100%|██████████| 300/300 [00:53<00:00,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS - Train Loss: 0.0262, Train Accuracy: 0.9912, Val Loss: 0.0499, Val Accuracy: 0.9858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model_ner.to(device)\n",
    "bert_model_pos.to(device)\n",
    "\n",
    "num_epochs=3\n",
    "# Main training and validation loop for NER\n",
    "print(\"Training and Evaluating NER Model\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - NER\")\n",
    "\n",
    "    # Training\n",
    "    train_loss_ner_bert, train_acc_ner_bert = train_model(bert_model_ner, optimizer_ner, loss_fn, train_dataloader_ner, device)\n",
    "    # Validation\n",
    "    val_loss_ner_bert, val_acc_ner_bert = evaluate_model(bert_model_ner, loss_fn, val_dataloader_ner, device)\n",
    "\n",
    "    tqdm.write(f\"NER - Train Loss: {train_loss_ner_bert:.4f}, Train Accuracy: {train_acc_ner_bert:.4f}, Val Loss: {val_loss_ner_bert:.4f}, Val Accuracy: {val_acc_ner_bert:.4f}\")\n",
    "\n",
    "# Main training and validation loop for POS tagging\n",
    "print(\"\\nTraining and Evaluating POS Tagging Model\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - POS\")\n",
    "\n",
    "    # Training\n",
    "    train_loss_pos_bert, train_acc_pos_bert = train_model(bert_model_pos, optimizer_pos, loss_fn, train_dataloader_pos, device)\n",
    "    # Validation\n",
    "    val_loss_pos_bert, val_acc_pos_bert = evaluate_model(bert_model_pos, loss_fn, val_dataloader_pos, device)\n",
    "\n",
    "    tqdm.write(f\"POS - Train Loss: {train_loss_pos_bert:.4f}, Train Accuracy: {train_acc_pos_bert:.4f}, Val Loss: {val_loss_pos_bert:.4f}, Val Accuracy: {val_acc_pos_bert:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654ba5c-eca7-4918-856a-d2b1e26fdea4",
   "metadata": {},
   "source": [
    "#### Inference and Reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07e3b4ba-3b39-4b0f-94d0-505096a85022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack - NER: B-per, POS: NNP\n",
      "Obama - NER: I-per, POS: NNP\n",
      "visited - NER: O, POS: VBD\n",
      "London - NER: B-geo, POS: NNP\n",
      "in - NER: O, POS: IN\n",
      "2015 - NER: B-tim, POS: CD\n",
      "and - NER: O, POS: CC\n",
      "spoke - NER: O, POS: VBD\n",
      "at - NER: O, POS: IN\n",
      "the - NER: O, POS: DT\n",
      "University - NER: B-org, POS: NNP\n",
      "of - NER: I-org, POS: IN\n",
      "Cambridge - NER: I-org, POS: NNP\n",
      ". - NER: O, POS: PUNCT\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Define the sentence\n",
    "sentence = \"Barack Obama visited London in 2015 and spoke at the University of Cambridge.\"\n",
    "\n",
    "# Tokenize the sentence using BERT tokenizer\n",
    "inputs = bert_tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "# Perform inference for NER and POS tagging\n",
    "with torch.no_grad():\n",
    "    outputs_ner = bert_model_ner(input_ids, attention_mask=attention_mask)\n",
    "    outputs_pos = bert_model_pos(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get the predicted tags\n",
    "predictions_ner = torch.argmax(outputs_ner.logits, dim=2)\n",
    "predictions_pos = torch.argmax(outputs_pos.logits, dim=2)\n",
    "\n",
    "# Decode predictions into tags\n",
    "ner_tags = [idx2tag[idx] for idx in predictions_ner[0].cpu().numpy()]\n",
    "pos_tags = [idx2pos_tag[idx] for idx in predictions_pos[0].cpu().numpy()]\n",
    "\n",
    "# Decode tokens into words\n",
    "tokenized_words = bert_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Align words with their tags and handle special cases\n",
    "processed_sentence = []\n",
    "for word, ner_tag, pos_tag in zip(tokenized_words, ner_tags, pos_tags):\n",
    "    if word.startswith(\"[\"):\n",
    "        # Skip special tokens\n",
    "        continue\n",
    "\n",
    "    # Handle punctuation\n",
    "    if word in string.punctuation:\n",
    "        processed_sentence.append((word, \"O\", \"PUNCT\"))  # PUNCT is a generic POS tag for punctuation\n",
    "    else:\n",
    "        processed_sentence.append((word, ner_tag, pos_tag))\n",
    "\n",
    "# Print the words with their processed tags\n",
    "for word, ner_tag, pos_tag in processed_sentence:\n",
    "    print(f\"{word} - NER: {ner_tag}, POS: {pos_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "240107d8-a7bc-42ce-8477-7cf61af8ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         art       0.34      0.26      0.29        82\n",
      "         eve       0.46      0.25      0.32        53\n",
      "         geo       0.84      0.90      0.87      7485\n",
      "         gpe       0.96      0.95      0.95      3190\n",
      "         nat       0.76      0.31      0.44        42\n",
      "         org       0.74      0.69      0.72      4032\n",
      "         per       0.76      0.78      0.77      3443\n",
      "         tim       0.87      0.88      0.88      4034\n",
      "\n",
      "   micro avg       0.83      0.84      0.84     22361\n",
      "   macro avg       0.72      0.63      0.66     22361\n",
      "weighted avg       0.83      0.84      0.84     22361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate report for NER\n",
    "ner_report_BERT = predict_and_generate_report(bert_model_ner, val_dataloader_ner, idx2tag)\n",
    "print(\"NER Report:\\n\", ner_report_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f820c17f-83c8-4e39-a9bc-9599037ad1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           $       1.00      1.00      1.00       226\n",
      "           ,       1.00      1.00      1.00      6542\n",
      "           .       1.00      1.00      1.00      9557\n",
      "           :       0.91      0.76      0.83       156\n",
      "           ;       0.97      1.00      0.98        32\n",
      "          CC       1.00      1.00      1.00      4749\n",
      "          CD       1.00      1.00      1.00      4962\n",
      "          DT       1.00      1.00      1.00     19628\n",
      "          EX       0.99      0.98      0.99       121\n",
      "          IN       0.99      0.99      0.99     24221\n",
      "          JJ       0.95      0.97      0.96     15684\n",
      "         JJR       0.92      0.98      0.95       599\n",
      "         JJS       0.98      0.96      0.97       608\n",
      "         LRB       1.00      1.00      1.00       143\n",
      "          MD       1.00      1.00      1.00      1375\n",
      "          NN       0.99      0.97      0.98     29326\n",
      "         NNP       0.99      0.98      0.98     26286\n",
      "        NNPS       0.91      0.73      0.81       535\n",
      "         NNS       0.98      1.00      0.99     15312\n",
      "         PDT       0.77      0.92      0.84        26\n",
      "         POS       1.00      1.00      1.00      2204\n",
      "         PRP       1.00      1.00      1.00      2595\n",
      "        PRP$       1.00      1.00      1.00      1693\n",
      "          RB       0.98      0.96      0.97      4084\n",
      "         RBR       0.87      0.82      0.84       201\n",
      "         RBS       0.72      0.96      0.82        53\n",
      "          RP       0.90      0.95      0.92       491\n",
      "         RRB       1.00      1.00      1.00       143\n",
      "          TO       0.99      1.00      1.00      4692\n",
      "          UH       1.00      1.00      1.00         3\n",
      "          VB       0.98      0.98      0.98      4891\n",
      "         VBD       0.98      0.98      0.98      7790\n",
      "         VBG       0.97      0.99      0.98      3816\n",
      "         VBN       0.97      0.97      0.97      6416\n",
      "         VBP       0.96      0.99      0.98      3210\n",
      "         VBZ       0.99      0.99      0.99      5037\n",
      "         WDT       0.94      0.99      0.97       731\n",
      "          WP       1.00      1.00      1.00       526\n",
      "         WP$       1.00      1.00      1.00        13\n",
      "         WRB       1.00      1.00      1.00       456\n",
      "          ``       1.00      1.00      1.00       696\n",
      "\n",
      "    accuracy                           0.99    209829\n",
      "   macro avg       0.97      0.97      0.97    209829\n",
      "weighted avg       0.99      0.99      0.99    209829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate report for POS\n",
    "pos_report_BERT = predict_and_generate_report(bert_model_pos, val_dataloader_pos, idx2pos_tag, is_ner=False)\n",
    "print(\"POS Tagging Report:\\n\", pos_report_BERT)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
